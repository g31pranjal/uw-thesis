\chapter{Evaluation}
\label{c:evaluation}

We integrated our columnar storage and techniques into the in-memory GraphflowDB GDBMS. The version of the system we modified stored the edge and vertex properties in a row-oriented fashion as a sequence of variable-sized records indexed by their IDs and partitions the edges by labels and stores them as 8-byte vertex and edge IDs inside CSR. The goal of our experiments is two-fold. First, we show that using columnar storage and compression techniques reduces the memory consumption of the system significantly, by 3.5x, when storing a graph dataset generated by the popular LDBC social network benchmark. Second, we evaluate the query performance benefits and tradeoffs that these techniques provide. In particular, we organize our experiments as follows:
%  We show that redesigning the storage layer of \gls{gdbms} using columns that can harness the structure that exist in data  the graph can compact the storage by up to 3.5x.

\begin{enumerate}
	\item \textbf{Compression in Adjacency Lists:}  In Section~\ref{exp:adjacency-list-exp} we show the reduction in the size of our adjacency lists when applying the storage optimizations from Sections~\ref{c:columnar-storage} and~\ref{columnar-compression}. For each optimization, we state the cause of the reduction in size and query performance effect of the optimization.
		
	\item \textbf{Single-directional Property Pages:} In Section~\ref{exp:property-pages}, we compare the performance of storing edge properties in single-directional property pages, as compared to strictly-ordered single-directional property lists and unordered edge columns.
	
	\item \textbf{Vertex Columns for Single Cardinality Edges vs. CSR Adjacency Lists:}  In Section~\ref{exp:single-cardinality}, we show the effectiveness of storing single cardinality edges in vertex  columns as compared to in CSR format. 
	
	\item \textbf{Prefix Sum-based Null Compression:}  In Section~\ref{exp:prefixSum}, we evaluate the size  and performance tradeoff of compressing our columns with our prefix sum-based null compression scheme. We also compare our technique with the vanilla null compression technique mentioned in~\cite{abadi-sparse-col}.% to show the benefit of using a map instead of iterating over bit-strings.

	\item \textbf{List-based Processing vs Volcano-styled Query Execution:} In Section~\ref{exp:list-based}, we show the performance benefits of list-based processing over Volcano-styled processing.
	
\end{enumerate}

Noticeably missing in our evaluations is comparisons against the other \gls{gdbms} systems and conventional column stores, which we plan to perform as part of future work.

\section{Experimental Setup}

\noindent \textbf{Hardware Setup:} For all our experiments, we use a single machine that has two Intel E5-2670 @2.6GHz CPUs and 512 GB of RAM. The machine has 16 physical cores and 32 logical cores. We only use one logical core. We set the maximum size of the JVM heap to 500 GB and keep JVM's default minimum size.

\begin{figure}
	\centering
	\includegraphics[scale=0.42]{img/ldbc-schema}
	\captionsetup{justification=centering}
	\caption{\gls{ldbc} Graph schema. (Obtained from LDBC SNB specification document v0.3.2)}
	\label{fig:ldbc-schema}
\end{figure}

\noindent \textbf{Dataset:} We use the \gls{ldbc}~\cite{ldbc} dataset generator to generate synthetic graph datasets on which we evaluate all our experiments. \gls{ldbc} is a popular benchmark to generate large social graph data with attributes on nodes and edges. The generated social graph is highly structured which can be seen from the schema in Figure~\ref{fig:ldbc-schema}. In fact all of the edges and edge and vertex properties are structured according to our definition from Chapter~\ref{c:guidelines} but several properties and edges are very sparse. For example, property X on vertices with type X appears in less than X\% of the nodes with type X.\todo{Complete the X's.}  We generate the data at the scale factor of 100 (\gls{ldbc} 100) that consists of over 1.7 billion edges and 0.3 billion vertices. We refer to this dataset as LDBC100

\noindent \textbf{Query Workload:} We use a micro benchmark we generate that consists of 1-, 2-, and 3-hop path queries that optionally contain predicates on vertex and edge properties and aggregations. These queries serve as a stress test for evaluating access to the underlying storage, which our techniques optimize. 
%For instance, a simple 1-hop query tests more rigorously for access from adjacency lists than a query that involves filtering and aggregations. 

\section{Compression in Adjacency Lists}
\label{exp:adjacency-list-exp}

%We introduced several optimizations that can make use of the graph data's structure to drastically compact the representation of edges and vertices in the adjacency lists. 
In this experiment, we demonstrate the memory reduction we get from the columnar storage and compression techniques we described in this thesis that reduce the cost of storing (edgeID, neighborID) pairs in adjacency lists.% that exploits structure in graph data on LDBC100.  
% optimizations on \gls{ldbc} 100 by recording the memory utilization of the adjacency lists across different %configurations which we describe momentarily.
We create multiple configurations on GraphflowDB, each with a different set of optimizations. Below are the descriptions of the configurations we evaluate the memory usage on. Each configuration builds on top of previous in the list. 

\begin{enumerate}
	\item \texttt{GF-OLD:} This is our baseline configuration that represents edges and vertices in the adjacency list as 8-byte identifiers. All the edges are stored in the 2-level CSR structure and are not compressed.
	\item \texttt{+COLS}: Uses vertex columns from single cardinality edge labels. 
	\item \texttt{+NEW-IDS}: Introduces our new vertex and edge identification schemes.
	\item \texttt{+0-SUPR}: Implements leading 0 suppression in the components of vertex and edge IDs in adjacency lists.
	\item \texttt{+OMIT}: Omits neighbor vertex label and positional offsets of edge IDs when they can be inferred from the structure.
	\item \texttt{+NULL}: Implements prefix sum-based null compression on adjacency lists (or vertex columns for single cardinality edges).
\end{enumerate}

\begin{table}
	\centering
	\bgroup
	\setlength{\tabcolsep}{8pt}
	\def\arraystretch{1.2}%  1 is the default, change whatever you need
	\begin{tabular}{ |c|c|c|c|c|c|c| } 
		\hline
		& \texttt{GF-OLD} & \texttt{+COLS} & \texttt{+NEW-IDS} & \texttt{+0-CMPRS} & \texttt{+OMIT}& \texttt{+NULL} \\ 
		\hline \hline
		\multirow{2}{120pt}{Fwd. Adjacency Lists}& 38.25 & 33.25 & 27.22 & 16.35 & 11.14 & 10.53 \\ 
		& & \textbf{+1.15x} & \textbf{+1.22x} & \textbf{+1.66x} & \textbf{+1.47x} & \textbf{+1.06x} \\ 
		\hline
		\multirow{2}{120pt}{Bwd. Adjacency Lists}& 37.93 & 37.50 & 30.93 & 18.75 & 12.79 & 11.15 \\ 
		& & \textbf{+1.01x} & \textbf{+1.21x} & \textbf{+1.65x} & \textbf{+1.47x} & \textbf{+1.15x} \\ 
		\hline
		\multirow{3}{120pt}{Total (GB)\\Bytes Per Edge\\} & 76.18 & 70.75 & 58.15 & 35.10 & 23.93 & 21.68 \\ 
		 & 23.04 & 21.39 & 17.58 & 10.61 & 7.24 & 6.50 \\ 
                 & & \textbf{1.08x} & \textbf{1.31x} & \textbf{2.17x} & \textbf{3.18x} & \textbf{3.55x} \\  
%		\hline \hline
%		\multirow{3}{80pt}{Bytes per edge}& 23.04 & 21.39 & 17.58 & 10.61 & 7.24 & 6.50 \\ 
%		& & \textbf{1.08x} & \textbf{1.31x} & \textbf{2.17x} & \textbf{3.18x} & \textbf{3.55x} \\ 
		\hline
	\end{tabular}
	\egroup
	\captionsetup{justification=centering}
	\caption{Memory utilization (in GB) by Adjacency lists of  LDBC100 when adding our optimizations one at a time. Each column $i$ indicates an optimization $i$ and in the rows for forward and backward lists indicate the additional reduction factor of applying optimization $i$ on top of the previous optimizations to the left of $i$. In contrast, in the row on total memory consumption/bytes per edge, each column $i$ indicates the cumulative reduction factor (compared to \texttt{GF-OLD}) of applying all optimizations from left until $i$.}
	\label{tbl:mem1}
\end{table}

\begin{figure}
	\centering
	\includegraphics[scale=0.75]{img/opti-breakup}
	\captionsetup{justification=centering}
	\caption{Breakup of memory gains by applying different optimizations on Adjacency Lists of \gls{ldbc} scale factor 100 dataset.}
	\label{fig:opti-breakup}
\end{figure}

Table~\ref{tbl:mem1} shows how much memory (in GB) the adjacency lists take when storing the edges of LDBC100 across different configurations.  Figure~\ref{fig:opti-breakup} gives the breakup of memory gain per optimization. Memory gain in \texttt{+COLS} is attributed to the fact that 8 out of 15 edge labels are single cardinality and stored in vertex columns in at least one direction. \texttt{+NEW-IDS} gains ({\raise.17ex\hbox{$\scriptstyle\sim$}}4 bytes per edge) by storing edges in smaller than 8 bytes, while \texttt{+OMIT} gains ({\raise.17ex\hbox{$\scriptstyle\sim$}}3 bytes per edge) from not storing edge's positional offsets and neighbour's vertex label for 10 out of 15 edge labels. We see modest benefits in \texttt{+NULL} ({\raise.17ex\hbox{$\scriptstyle\sim$}}0.75 bytes per edge) since empty adjacency lists are infrequent in LDBC100.

All of these optimizations also improve query performance, with the exception of \texttt{+NULL}, which incurs a modest query slow down. We will evaluate the performance gains and tradeoffs of \texttt{+COLS} and \texttt{+NULL} are evaluated in Sections~\ref{exp:single-cardinality} and ~\ref{exp:prefixSum} respectively. 
We will not evaluate the benefits of leading 0 suppression but we note that this also improves performance because we do not have to decode a particular value and hence, and have modest gains because we copy less data into tuples from the adjacency lists.

%Further, the memory utilization of \gls{ldbc} 300 (5.27B edges, 0.6B vertices) with our set of optimizations was 63.45GB. In all, we are able to scale the new GraphflowDB (with our new storage layer) to store up to 10.5B edges, along with vertex and edge properties, of \gls{ldbc} data on a single machine with 500GB of main memory. This is {\raise.17ex\hbox{$\scriptstyle\sim$}}15x increase in scalability compared to old GraphflowDB with unoptimized storage.

\section{Effectiveness of Single-Directional Property Pages}
\label{exp:property-pages}

In this experiment, we show the benefits of keeping the edge properties grouped in single-directional property pages compared to using edge columns that store the edge properties in an unordered way. 
Recall that one alternative design we described was using  single-directional property lists to store edge properties. This design had the advantage that edge properties would be kept in exactly the same order as they appear in adjacency lists, but it can make updates significantly slower, specifically for systems that keep edges in sorted order. However, this design may still be a viable design for some GDBMSs and as a point of reference we also evaluate its benefits. 
%We also demonstrate that keeping edge properties in property pages instead of single-directional property lists is a reasonable trade-off as we lose modest performance. but significantly win over edge columns. 
To test the performance benefits of single-directional property pages and lists, we configure GraphflowDB  in 3 different ways (all using our new ID schemes):

\begin{enumerate}
	\item \texttt{EDGE COLS:} Stores edge properties in edge column in the order they were inserted into the database, so properties of edges in a particular adjacency list (forward or backward) can appear anywhere in this column. We ensure that the insertion order is random. % in  If there are $|E|$ many e For example, the edge properties of label \texttt{knows} are stored in columns that are oblivious to the location of \texttt{knows} edges in either the forward or backward adjacency lists. 
	\item \texttt{PROP LISTS:} Edge properties are stored in the single-directional property lists. We pick the forward list for $n-n$ multiplicity edges. Therefore these property lists mimic the forward adjacency lists and gives sequential reads when edge properties of a forward adjacency list are read.
	
	\item \texttt{PROP PAGES:} Edge properties are stored in pages by combining $n=128$ property lists of the previous solution and appear in insertion order. This solution provides close-by reads when edge properties of a forward adjacency list are read.
\end{enumerate}

\begin{table}
	\centering
	\bgroup
	\setlength{\tabcolsep}{8pt}
	\def\arraystretch{1.2}%  1 is the default, change whatever you need
	\begin{tabular}{ |c|c|c|c| } 
		\hline
		& \textbf{1-hop} & \textbf{2-hop} & \textbf{3-hop} \\
		\hline \hline
		\texttt{EDGE COLS}& 3.42 & 308.18 & 966.04\\ 
		\hline 
		\multirow{2}{*}{\texttt{PROP PAGES}} & 1.22 & 152.89 & 512.24 \\ 
		& \textbf{2.80x} & \textbf{2.03x} & \textbf{1.87} \\ 
		\hline
		\multirow{2}{*}{\texttt{PROP LISTS}}& 1.03 & 125.65 & 372.44 \\ 
		& \textbf{3.32x} & \textbf{2.42x} & \textbf{2.59}\\ 
%		\hline \hline
%		\texttt{PROP PAGES} vs \texttt{PROP LISTS} & 1.19x & 1.22x & 1.37x \\
		\hline
	\end{tabular}
	\egroup
	\captionsetup{justification=centering}
	\caption{Runtime (in sec) of k-hop queries for different configurations of edge property storage on \gls{ldbc} 100.}
	\label{tbl:mem2}
\end{table}

As our workload, we use 1- 2- and 3-hop queries that use the \texttt{knows} edge label of the \gls{ldbc} schema (Figure~\ref{fig:ldbc-schema}) that compare each query edge's \texttt{creationDate} property to be greater than the previous edge's. 1-hop and 2-hop queries run for all vertices of \texttt{person} while we run 3-hop for only 25000 vertices to make the queries faster. For each query, we only consider the plan that matches vertices from left to right in the forward direction. This way, we are ensure cache locality for \texttt{PROP-LISTS} and \texttt{PROP-PAGES}. Table~\ref{tbl:mem2} shows the performance of queries on 3 configurations. We observe that the queries benefit significantly from localizing edge properties in the storage. Compared to \texttt{EDGE COLS}, we see up to 2.8x improvement for \texttt{PROP PAGES} and up to 3.32x improvements for \texttt{PROP LISTS}. Recall also that using \texttt{PROP PAGES} or \texttt{PROP LISTS} also reduces our storage because the amount of bytes required to identify edges within a list or a property page is smaller than the bytes needed to identify an edge within an entire edge column. In a system that uses fixed size IDs, which in-memory systems optimized for performance should do, 8 bytes is needed to identify edges in billion-scale graphs while 4 bytes is sufficient to identify edges within a list or property page. Finally, note that the benefits we get from \texttt{PROP PAGES} will depend on $n$, in particular as $n$ approaches 1, \texttt{PROP PAGES} design reduces to \texttt{PROP LISTS}. We have not performed this experiment but as $n$ gets smaller we expect the performance differences between \texttt{PROP LISTS} and \texttt{PROP PAGES} to reduce.

% In addition, using property lists or pages allows us to use positional offsets that need to identify edges within a list of a property page (so 128 lists). 
%The benefits decrease with the size of query because the last operators in a query plan flush the pages from which earlier operators read. However, both property lists and pages still guarantee benefits over unordered \texttt{EDGE COLS}. On the other hand, storing the edge properties in the update-friendly property pages only slightly degrades the performance of queries (1.19-1.37x). This depends on the value of $n$, i.e, the number of property lists that we put together on a page. The average number of \texttt{knows} edges per \texttt{person} vertex is {\raise.17ex\hbox{$\scriptstyle\sim$}}44 edges while $n=128$. A lower $n$ means that properties of edges in an adjacency lists appear closer in the page and hence, provide more localized reads.

\section{Vertex Property Columns for Single Cardinality Edges vs. 2-level CSR Adjacency Lists}
\label{exp:single-cardinality}

Storing single cardinality edges in vertex columns ensure two straight benefits, 1) direct access without indirection into CSR, and 2) does not need to store offsets of the CSR. We showed the memory gains of storing edges in vertex columns on \gls{ldbc} 100 in Section~\ref{exp:adjacency-list-exp}, though these benefits were cumulative of the entire adjacency lists. In this experiment we compare the performance of a query with single cardinality edges with respect to, 1) using vertex column vs 2-level CSR adjacency lists to store edges, and 2) if the end structure is null compressed or not. We explicitly test performance on the compressed data structures since having edge label cardinality \texttt{0..1} rather than \texttt{1} is common in real-world graphs. Hence, we create 4 configurations of GraphflowDB to run our queries on:

\begin{enumerate}
	\item \texttt{V-COL-UNC:} Single cardinality edge label edges are stored are in vertex columns and are not compressed. This is equivalent to \texttt{+OMIT} configuration in Section~\ref{exp:adjacency-list-exp}.
	\item \texttt{CSR-UNC:} Single cardinality edge label edges are stored in 2-level CSR adjacency lists and are not compressed.
	\item \texttt{V-COL-C:} PrefixSum-based \texttt{NULL} Compressed version of \texttt{V-COL-UNC}. This is equivalent to \texttt{+NULL} configuration in Section~\ref{exp:adjacency-list-exp}.
	\item \texttt{CSR-C:} PrefixSum-based \texttt{NULL} Compressed version of \texttt{CSR-UNC}.
\end{enumerate}

The workload consists of simple 3 k-hop queries, $k=1,2,3$, on the \texttt{replyOf} edge between \texttt{comment} vertices in the \gls{ldbc} schema. The \texttt{replyOf} edge label has \texttt{n-1} cardinality, hence, we keep the forward edges as a special property of \texttt{comment} vertex label. Moreover, our workload queries do not do any predicate evaluation and the final output of the query is an aggregated count. This assures that \texttt{JOIN} operation in the query plan is the only dominant operation. Again, for each query, we evaluate only on the plan that matches the vertices sequentially and joins in the forward direction.

\begin{table}
	\centering
	\begin{subtable}{1\textwidth}
		\centering
		\bgroup
		\setlength{\tabcolsep}{8pt}
		\def\arraystretch{1.2}%  1 is the default, change whatever you need
		\begin{tabular}{ |c|c|c|c|c| }
			\hline
			& \textbf{1-hop} & \textbf{2-hop} & \textbf{3-hop} & \textbf{Memory (in MB)} \\ 
			\hline \hline
			\texttt{CSR-UNC}& 7.03 & 9.13 & 9.60 & 1266.56 \\ 
			\hline
			\multirow{2}{*}{\texttt{V-COL-UNC}}& 4.34 & 5.80 & 5.85 & 839.93 \\ 
			& \textbf{1.62x} & \textbf{1.57x} & \textbf{1.64x} & \textbf{1.51x} \\ 
			\hline
		\end{tabular}
		\egroup
		\captionsetup{justification=centering}
		\caption{Uncompressed}
		\label{tbl:s1}
	\end{subtable}
	\begin{subtable}{1\textwidth}
		\centering
		\bgroup
		\setlength{\tabcolsep}{8pt}
		\def\arraystretch{1.2}
		\begin{tabular}{ |c|c|c|c|c| } 
			\hline
			& \textbf{1-hop} & \textbf{2-hop} & \textbf{3-hop} & \textbf{Memory (in MB)} \\ 
			\hline \hline
			\texttt{CSR-C}& 7.78 & 10.40 & 11.23 & 905.23 \\ 
			\hline
			\multirow{2}{*}{\texttt{V-COL-C}}& 5.23 & 8.28 & 8.41 & 478.86 \\ 
			& \textbf{1.49x} & \textbf{1.26x} & \textbf{1.34x} & \textbf{1.89x} \\ 
			\hline
		\end{tabular}
		\egroup
		\captionsetup{justification=centering}
		\caption{Null Compressed}
		\label{tbl:s2}
	\end{subtable}
	\captionsetup{justification=centering}
	\caption{Vertex property columns vs. 2-level CSR adjacency lists for storing single cardinality edges: Query runtime (in sec) and Memory usage (in MB)  }
\end{table}

Tables~\ref{tbl:s1} and ~\ref{tbl:s2} shows the result of queries on uncompressed and null compressed configurations respectively. 	We get the maximum speed up of 1.62x in uncompressed variants, while its lesser for compressed couterparts with the maximum 1.49x. The last column of the tables report the size of the adjacency lists or vertex column storing \texttt{replyOf} edges. Here, vertex column uses half as much space as adjacency lists, when the data is kept compressed. In \gls{ldbc} 100, out of {\raise.17ex\hbox{$\scriptstyle\sim$}}220M \texttt{comment} vertices, approx. 50\% have empty forward adjacency list, i.e, do not have an outward \texttt{replyOf} edge. This is reflected in vertex columns between \texttt{V-COL-UNC} and \texttt{V-COL-C}, as the memory reduces to half (839.93MB vs 478.86MB), unlike their CSR counterparts that stores offsets as extra and thus, compression reduces memory by only 25\%.

\section{Effectiveness of PrefixSum-based Null Compression Technique}
\label{exp:prefixSum}

PrefixSum-based null compression technique is used in our system to compress both, sparse columns and empty adjacency lists. We evaluate our technique in 2 settings, 1) Storage and random access efficacy against uncompressed and vanilla NULL compressed columns, and 2) query performance on compressed and uncompressed vertex columns and adjacency lists.

\subsection{Stress Test} 

\begin{figure}
	\hspace*{-25pt}
	\begin{subfigure}{0.55\textwidth}
		\centering
		\includegraphics[scale=0.30]{img/pref-space}
		\captionsetup{justification=centering}
		\caption{Memory usage}
		\label{fig:pref-space}
	\end{subfigure}
	\begin{subfigure}{0.55\textwidth}
		\centering
		\includegraphics[scale=0.30]{img/pref-perf}
		\captionsetup{justification=centering}
		\caption{Performance on random accesses}
		\label{fig:pref-perf}
	\end{subfigure}
	\captionsetup{justification=centering}
	\caption{Memory and performance on random accesses for Uncompressed, prefixSum-based NULL compressed and vanilla NULL compressed columns.}
	\label{fig:pref-stress}
\end{figure}

The goal of this experiment is to compare uncompressed, prefixSum-based null compressed and vanilla null compressed (as implemented in ~\cite{abadi-sparse-col}) columnar data-structure for memory usage and performance on random reads. For this, we use the \texttt{creationDate} property of 220M \texttt{comment} vertices in \gls{ldbc} 100 and create multiple versions of it, each with different \%age of non-\texttt{NULL} values. On each version, we do the necessary compression and measure the time taken to do 200M access to random locations in the that column. 

Figures~\ref{fig:pref-space} and~\ref{fig:pref-perf} shows memory usage and performance on 200M random read queries respectively, for uncompressed, prefixSum-based \texttt{NULL} compressed and vanilla \texttt{NULL} compressed column of \texttt{creationDate} property of \texttt{comment} vertex label. We omit the performance number of vanilla \texttt{NULL} compression as they were significantly higher than the other two ($>$20x). Our prefixSum-based \texttt{NULL} compression technique is only marginally over vanilla \texttt{NULL} comression in memory usage (we take 2-bit for each element vs. 1-bit overhead in vanilla compression). However, introducing prefixSums and map lookups, in place of iterations over bit-strings, makes the performance of random access comparable on prefixSum-based \texttt{NULL} compressed column comparable to uncompressed (1.38x slower at worst). In fact, our compression comes out to be faster than uncompressed for very sparse columns ($<30$ non-\texttt{NULL} values). This is because testing for \texttt{NULL} at a location is a constant time operation.

\subsection{Query Performance}

Tables~\ref{tbl:s1} and~\ref{tbl:s2} already compares reading edges from compressed and uncompressed adjacency lists and vertex columns. Reading edges from CSR adjacency lists and vertex columns that are \texttt{NULL} compressed by our technique are on average 1.14x and 1.3x slower than their uncompressed variant respectively. However, for 50\% \texttt{NULL} values, they gain 50\% and 25\% in storage respectively. 

\begin{table}
	\centering
	\bgroup
	\setlength{\tabcolsep}{8pt}
	\def\arraystretch{1.2}%  1 is the default, change whatever you need
	\begin{tabular}{ |c|c|c|c| } 
		\hline
		& \textbf{1-hop} & \textbf{2-hop} & \textbf{3-hop} \\
		\hline \hline
		\texttt{V-COL-UNC}& 12.78 & 14.47 & 14.95\\ 
		\hline
		\multirow{2}{*}{\texttt{V-COL-C}}& 14.65 & 16.00 & 16.98 \\ 
		& \textbf{1.15x} & \textbf{1.11x} & \textbf{1.14x}\\ 
		\hline  
	\end{tabular}
	\egroup
	\captionsetup{justification=centering}
	\caption{Runtime (in sec) of k-hop queries on compressed and uncompressed vertex column on \gls{ldbc} 100.}
	\label{tbl:s3}
\end{table}

We next evaluate accessing vertex properties from compressed and uncompressed vertex columns on configurations \texttt{V-COL-C} and \texttt{V-COL-UNC} respectively. Edge storage is not compressed in these configurations. We run a workload derived from that in Section~\ref{exp:single-cardinality} consisting of k-hop queries over \texttt{replyOf} edges. These queries are extended to include a condition that a \texttt{comment} $b$ is made on another \texttt{comment} $a$ such that $b$ has a \texttt{creationDate} that is within $\delta$ timespan of $a$'s. Table~\ref{tbl:s3} shows the numbers from the experiment which suggest that using compressed storage only slows down the query by 1.14x which is a reasonable trade-off when compared to compact storage. We can expect the slow down factor to be always intermediary less than those we reported in the stress test.

\section{List-based Processing vs. Volcano-styled Query Execution}
\label{exp:list-based}

We compare list-based query processing to the conventional volcano-styled processing on several path queries.






















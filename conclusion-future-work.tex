\chapter{Conclusion and Future Work}
\label{c:conclusion-future-work}

We studied the applicability and integration of column-oriented storage and query processing techniques in the context of \gls{gdbms}, which presents several significant deviations from an \gls{rdbms}. Consequently, the aim of this thesis is not introduce a new form of storage and query processing techniques, but to adapt an already existing set of work to a new setting. We show that the conventional techniques used in column-oriented \gls{rdbms} cannot be passed on to a \gls{gdbms} directly and hence, we redesign them to suit our requirements. 

At the core of our work is the fact that graph data is not completely unstructured and that this data is accessed in definite patterns during a graph query processing. In fact, there exists different types of structures in a graph, that constitutes a soft schema, that every graph data adheres to. In this thesis, we propose efficient columnar data structures and powerful optimizations that compacts the in-memory storage of property graph data by exploiting this structure in graph data. Our columnar data structures stores data closely-packaged and provides efficient constant-time random access with decoding, which is an essential requirement in the context of \gls{gdbms}. Moreover, we order the data, in adjacency lists and single-directional property pages, in a way that helps achieve cache locality wherever possible. We also introduce list-based query processing to execute queries, which is a mix of traditional volcano-styled processing, that is prevalent in \gls{gdbms}, and vectorized processing, which is used in column stores. The list-based processing adds operators that operate on an entire adjacency list in a single execution thereby getting cache benefits from reading together from ordered placement of data.

As graph data can be highly sparse and inconsistent, we compress our structures, to avoid keeping \texttt{NULL}s or empty adjacency lists, using the redesigned \texttt{NULL} compressing technique that allows for random look-ups on compressed data. Our repurposed \texttt{NULL} compression allows access times comparable to uncompressed case while still. We also achieve compression on adjacency lists by using a new vertex and edge ID scheme which we adopt for simplifying property look-ups. Our new ID scheme can represent an edge and its neighbouring vertex in adjacency list by as less as 4 bytes.

We now outline some potential directions of future work:
\begin{itemize}
	\item \textbf{Optimizing unstructured graph data:} Evidently left out in our work is to optimize the system for storing unstructured data that appears erratically throughout in case of graph data. A naive approach to handling such data is to keep them in variable-width records and structures like linked-list which are update-friendly. However, such a storage requires decoding or is distributed randomly in memory, respectively. Most of the existing \gls{gdbms} follows a close approach by assuming the data has not structure at all. Though difficult, a possible line of future work can be to research efficient data structures that makes maintenance and accessing of unstructured data more efficient. 
	
	\item \textbf{Factorized processing:} Another interesting extension could be to study factorized processing in the context of \gls{gdbms}. Our current list-based processing is a simpler form of factorized processing which is limited in capability, i.e, it can only generate plans that has \texttt{LIST JOIN} only if no other \texttt{JOIN} is dependant on it for input values or it is the last join in the query plan. On the other, a complete factorized query processor will be able to perform \texttt{LIST JOIN}s at all levels and produce the most succinct output of the query.
	
	\item \textbf{Comparison to other systems: }An interesting evaluation that we left out is to compare GraphflowDB, with our columnar data structures and techniques, to the existing commercial as well as \gls{gdbms}s on a well-defined workload from a popular benchmark, like \gls{ldbc}. This will test the overall efficiency in storage and performance of our changes on more practical queries. Extending to it, comparing to column-oriented \gls{rdbms}, like monetDB, duckDB etc., can be insightful too. This will also test our list-based processing technique against the vectorized processing that column stores use. 
	
\end{itemize}



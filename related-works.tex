\chapter{Related Work}
\label{c:related-works}
?
This thesis studied the integration of columnar storage and query processing techniques to GDBMSs. We review related work on column-oriented relational systems and other storage and compression techniques that have been designed for GDBMSs and RDF engines. There is an extensive literature on different storage and compression techniques designed for specific graphs, which we do not cover here. We refer the interested readers to reference \cite{sahu:survey, besta2018survey} for a survey on the lossless compression techniques from literature. These techniques focus on compressing the topology of the graph and broadly can achieve high compression rates for special types of graphs, e.g., Erd\H{o}s R\'enyi graphs or web graphs, but require decompression while accessing adjacencies. Therefore they are less applicable for an in-memory GDBMS that the techniques we considered in this thesis.
?
% on columnar storage and compression for GDBMSs and graph query processing. Section~\ref{graph-compression} discusses works in representing graphs and graph databases for a number of use cases. Section~\ref{column-storage} talks about existing solutions employing columnar storage and compression in \gls{rdbms} and RDF databaes. 
?
\section{Storage and Compression Techniques in Column-Oriented Relational Systems}
\label{sec:column-stores-overview}
?
Column-oriented RDBMSs are designed primarily for OLAP applications that are analytics heavy that perform aggregations over entire tables or subsets of tuples. At a high-level these systems store the relations in disk pages as a set of separate columns, instead of a set of rows. The column $c$ value of a particular row $r$ can be found using a positional offset, using the row ID of $r$, in the page that stores column $c$. The row IDs are referred to as surrogate keys. Prior to the emergence of column stores, such as C-Store~\cite{c-store}, MonetDB~\cite{monet-2decades}, and VectorWise~\cite{boncz-vectorwise, boncz-vectorwise1},  prior work, such as PAX~\cite{pax}, had used columnar storage inside in the context of row-oriented systems. PAX was a storage design that stores a set of rows in each disk page, but organizes the rows inside each page in a columnar format. 
?
Many prior work on column stores introduced a set of columnar storage, compression, and query processing techniques. These techniques include: positional offsets (also called virtual IDs) to directly access values in columns for different rows, columnar compression schemes, vector-oriented query processing, late materialization, and direct operation on compressed data among others. A detailed survey of these techniques can be found in reference~\cite{design-imp-book}, which reviews the techniques introduced in C-Store, MonetDB, and VectorWise.  This thesis studied how to directly integrate these techniques to in-memory GDBMSs or modify and adapt them so that they can be integrated into in-memory GDBMSs. The most relevant techniques that we identified and integrated are:  (i) a set of columnar storage structures to store different components of property graphs (shown in Table~\ref{tbl:summ}); (ii) vertex and edge ID schemes that allow direct positional offsets into these columns; (iii) compression schemes that include dictionary encoding, zero suppression, compressed edge and vertex IDs, and a null and empty-list compression scheme;  and (iv) list-based processing which is a hybrid between Volcano-style and vector-oriented processing. 
?
For disk-based GDBMSs, other techniques, in particular other compression schemes, such as run-length encoding to compress vertex and edge properties, or integer compression schemes from reference~\cite{lemire:integer} to compress adjacency lists might also be beneficial. For an in-memory systems these techniques can increase the systems scalability but they will also decrease performance as data needs to be decompressed before accessing. Except for null-compression, our other compression schemes do not decrease an in-memory \gls{gdbms}'s performance as they do not require decompression. Null compression slows down query execution but not significantly, so offers a good performance and space trade-off. Whether or not \gls{gdbms}-specific versions of other compression schemes can be developed for in-memory GDBMSs is an interesting research direction.
?
% are known to have their own set of optimizations and processing techniques that makes it highly efficient in storage and processing, like block processing, compression etc. \cite{col-vs-row, design-imp-book}. C-Store \cite{c-store} is an updatable column storage system that keeps multiple copies of columns sorted differently and uses positional offsets in a column to process data. Building on it, \cite{abadi-col-comp, abadi-sparse-col} talks about light-weight compression and execution over compressed data in C-Store. MonetDB \cite{monet-2decades} assumes similar vertical partitioning into columns \texttt{(BAT)} but follows a different execution stratetegy by means of binary opeations on BAT. MonetDB benefits from tight for-loops and bulk processing that give then good cache locality and avoids instruction misses. Even though our idea of vertical partitioning is in line with these works, our solution is native for \gls{gdbms} and hence organizes and executes process differently.
?
\section{Storage and Compression for GDBMSs and RDF Systems}
\label{sec:gdbms-storage-and-compression}
?
There are several existing native GDBMSs, whose internals have been described in technical papers or documents. These systems adopt a columnar structure only for storing the topology of the graph. This is done either by using a variant of vanilla adjacency list format or CSR. These systems use other row-oriented structures, such as property stores that store a sequence of key-value properties, or a separate key-value store to store vertex and edge properties. For example, Neo4j~\cite{neo4j} represents the topology of the graph in adjacency lists that are partitioned by edge labels and stored in linked-lists, where each edge record points to the next that is not necessarily stored consecutively in disk. Similarly, the property of each vertex and edge are stored in a linked-list in an unstructured manner, where each property record points to the next property record and encodes the key, data type, and value of the property. This can be seen as adopting a row-oriented storage for properties. Similarly, JanusGraph \cite{janusgraph} stores its edges in adjacency lists partitioned by edge labels and properties as a consecutive key-value pairs (so in row-oriented format). JanusGraph uses variable-length encoding when storing edges in the adjacency lists. Instead, we use fixed-length encodings in our compression schemes. DGraph~\cite{dgraph} uses key-value store to hold adjacency lists as well as properties.  We are unaware of any compression schemes adopted by Neo4j and DGraph. All of the above native GDBMSs adopt Volcano-style processors. In contrast, our design adopts columnar structures for vertex and edge properties and a list-based processor. Our storage techniques and list-based processing allows our system to benefit more from data locality. In addition we improve on these designs by more compressed edge and vertex ID representation (these systems use 8 bytes for each ID) and null compression.
?
There are also several GDBMSs that are developed directly on top of an RDBMS or another database system. For example Oracle Spatial and Graph~\cite{oracle} supports a property graph model using the PGQL language~\cite{pgql} that is built on top of Oracle database. SAP's graph database~\cite{hana-graph} is developed on top of HANA. These systems can benefit from the columnar techniques provided by the underlying RDBMS but the underlying RDBMS techniques are not optimized for graph storage and queries. For example, SAP's graph engine uses SAP HANA's columnar-storage to store vertex and edge tables but these columns do not have CSR-like structures for indexing edges of each vertex. Similarly existing RDBMSs do not implement null compression schemes similar to our prefix-sum-based scheme that allow constant-time access to arbitrary column values.
?
RedisGraph~\cite{redisgraph} is a system that stores its adjacency lists and properties inside the Redis~\cite{redis}, a distributed in-memory key value database. Though RedisGraph's latest release we are aware of runs only in single nodes. RedisGraph has a query processor that is based on performing linear algebra operations. The system converts Cypher queries into a sequence of linear-algebra based operators, which are executed using the GraphBLAS~\cite{graphblas} linear algebra library. This can be seen as performing column-oriented processing, as entire columns, which are stored in, are processed during query evaluation. We have not benchmarked our system against RedisGraph to evaluate the efficiency of this approach. This is an interesting direction we have left for future work.
?
ZipG~\cite{zipg} is a distributed compressed storage engine for property graphs that can answer queries to retrieve adjacencies as well as vertex and edge properties. ZipG is based on a compressed data structure called Succinct~\cite{succinct}. Succinct stores semi-structured data that is encoded as a set of key and list of values. For example, a node $v$'s properties can be stored with the $v$'s ID as the key and a list of values, corresponding to each property. The properties are distinguished through special delimiter characters ZipG maintains. Edge properties and adjacency lists can be encoded in a similar fashion. All of this data is encoded in flat files in a sorted manner by keys. Succinct then compresses these files using suffix arrays and several secondary level indices based on taking samples of key-value properties to access different records. Although the authors report achieving good compression rate, unlike our structures, access to a particular record is not constant time and requires accessing secondary indexes followed by a binary search. Therefore, this type of compression provides slower access than our structures. 
?
Reference \cite{compact-rep-graph} describes a k2-tree-based adjacency lists storage, while properties are represented either as lists or in k2-trees depending on the number of unique values. Our columnar structures differ from that in \cite{compact-rep-graph} in two ways: (i) we use positional offsets to access property values; and (ii) we arrange edge properties into pages to get sequential access.
?
Several RDF systems also use columnar structures to store RDF databases, which consist of a set of (subject, predicate, object) triples. Reference \cite{rdf-vertical} stores data in a set of columns, where each column store a set of (subject, object) pairs for each unique predicate. This is similar to partitioning the edges based on their labels in property graphs. However, this storage is not as optimized as the standard storages in GDBMSs, e.g., the edges of a particular object is not stored in native CSR or adjacency list format. Hexastore \cite{hexastore} improves on the idea of predicate partitioning by defining a column for each RDF element (subject, predicate or object) and sorting the column in 2 possible ways in B+ trees. This is similar but not as efficient as double indexing of adjacency lists in GDBMSs. RDF-3X \cite{rdf-3x} is an RDF system that stores a large triple table that is indexed in 6 B+ tree indexes over each column. Similarly, this storage is not as optimized as the native graph storages found in GDBMSs.
?
Another set of approaches \cite{comp-rdf, rbcomp, hdt} represent RDF data compactly by making use of the underlying structural patterns to formulate set of rules that can encodes multiple entities together. While, \cite{k2triples, ik2trees} aims at optimizing the topology of RDF data by representing it in data-structures similary to k2-trees \cite{k2trees}, that represents adjacency matrix as trees that are essentially NULL pruned. Our work too emphasizes on using structure in graph data but at the same time aims at improving query performance with a more succinct representation of data in memory.
?
Finally, several prior work has introduced novel storage techniques for storing graphs in GDBMSs or analytics systems. These works primarily focus on storing the topology of the graph and adopt variants of CSR format~\cite{yale}. LLAMA is a data structure based on CSR for storing adjacency lists for a write-heavy system. The data structure is designed to provide multi-version support when applications require accessing different versions of adjacency lists. Our focus in this thesis is read heavy queries and our data structures are not optimized for write-heavy workloads. Similarly STINGER~\cite{stinger} is a system that describes a data structure to store the topology of a graph also under write-heavy workloads . At a high-level the data structure adopts a vanilla adjacency list format that is divided by different edge labels, where each list is accessible using vertex IDs. Lists are stored in a linked list of blocks that allow very fast updates in a shared memory system. DISTINGER~\cite{distinger} adopts STINGER's structure to a distributed setting. These techniques are complementary to our work and our edgeID and vertex ID schemes and columnar structures for storing the graph topology can be integrated into these structures to benefit from their functionalities, such as multi-version support. 
